{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from DB import DB\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from sortedcontainers import SortedListWithKey\n",
    "import statistics\n",
    "import requests\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from xml.etree import ElementTree\n",
    "from auth import AzureAuthClient\n",
    "import requests\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_collection(argv):\n",
    "    database = DB(argv[0], argv[1])\n",
    "    collection = database.get_collection(argv[2])\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_results(collection, query, projection):\n",
    "    #cursor_it = collection.find(query,projection)\n",
    "    #cursor_it = collection.find()\n",
    "    cursor_it = collection.find({'constituent':'BMW'},{\"_id\":-1, \"id\":1, \"user.followers_count\":1,\"favorite_count\":1, \n",
    "                    \"retweet_count\":1,\"text\":1, \"processed_text\":1},limit=80000)\n",
    "    return list(cursor_it)\n",
    "'''\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analytics(cursor:list):\n",
    "    vocabulary = defaultdict(int)\n",
    "    top_retweeted = SortedListWithKey(key=lambda x: x[\"retweet_count\"])\n",
    "    top_fav = SortedListWithKey(key=lambda x: x[\"favorite_count\"])\n",
    "    top_followed = SortedListWithKey(key=lambda x: x[\"user\"]['followers_count'])\n",
    "    prices = []\n",
    "    \n",
    "    for tweet in cursor:\n",
    "        flag = False #look for prices\n",
    "        #clues = [\"bearish\", \"bullish\",\"hold\",\"stock\",\"share\",\"price\", \"EUR\", \"€\"]\n",
    "        #intersection_1 = [word for word in clues if word in tweet['text'].lower()]\n",
    "        #intersection_1 = [word for word in tweet[\"processed_text\"] if word in clues]\n",
    "        # intersection_2 = [word for word in tweet[\"processed_text\"] if word in [\"€\", \"EUR\", 'eur']]\n",
    "        intersection_1 = True\n",
    "        text = tweet[\"processed_text\"]\n",
    "    \n",
    "        if intersection_1:\n",
    "            if \"bmw\" in text:\n",
    "                flag = True\n",
    "                #print(tweet['text'])\n",
    "    \n",
    "        for word in tweet[\"processed_text\"]:\n",
    "            if flag:\n",
    "                try:\n",
    "                    number = float(word)\n",
    "                    if number < 150 and number > 50:\n",
    "                        prices.append(number)\n",
    "                except:\n",
    "                    pass\n",
    "         \n",
    "            vocabulary[word] += 1\n",
    "    \n",
    "        top_retweeted.add(tweet)\n",
    "        if len(top_retweeted) > 200:\n",
    "            top_retweeted.pop(0)\n",
    "\n",
    "        top_fav.add(tweet)\n",
    "        if len(top_fav) > 100:\n",
    "            top_fav.pop(0)\n",
    "                \n",
    "        top_followed.add(tweet)\n",
    "        if len(top_followed) > 100:\n",
    "            top_followed.pop(0)\n",
    "    \n",
    "    return vocabulary, top_retweeted, top_fav, top_followed, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_prices(prices):\n",
    "\n",
    "    prices.sort(reverse=True)\n",
    "    total = sum(prices)\n",
    "    print(\"Average price: {}\".format(total/len(prices)))\n",
    "    print(\"Highest price: {}\".format(prices[0]))\n",
    "    print(\"Lowest price: {}\".format(prices[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_german_tweets(cursor, collection):\n",
    "    stop_words = set(stopwords.words(\"german\"))\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,reduce_len=True,strip_handles=False)\n",
    "    punct = string.punctuation\n",
    "    punct_1 = punct.replace('#', '')\n",
    "    punct_2 = punct_1.replace('@', '')\n",
    "    stop_words.update(punct_2)\n",
    "    stop_words.add('...')\n",
    "    \n",
    "    for tweet in cursor:\n",
    "        if 'processed_text_de' not in tweet:\n",
    "            tokens = tokenizer.tokenize(tweet['text'])\n",
    "            filtered_tokens = [word for word in tokens if not word in stop_words]\n",
    "            filtered_no_url = [word for word in filtered_tokens if not 'http' in word]\n",
    "            collection.update_one({\"id\":tweet[\"id\"]}, \n",
    "                                { '$set': { 'processed_text_de': filtered_no_url}})\n",
    "            collection.update_one({\"id\":tweet[\"id\"]}, \n",
    "                                { '$unset': { 'processed_text': 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_tweet(text:str, language):\n",
    "    if language == 'en':\n",
    "        lang = 'english'\n",
    "    if language == 'de':\n",
    "        lang = 'german'\n",
    "\n",
    "    #Tokenize the tweet text\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,reduce_len=True,strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = string.punctuation\n",
    "    punct_1 = punct.replace('#', '')\n",
    "    punct_2 = punct_1.replace('@', '')\n",
    "    stop_words.update(punct_2)\n",
    "    stop_words.add('...')\n",
    "\n",
    "    filtered_tokens = [word for word in tokens if not word in stop_words]\n",
    "    filtered_no_url = [word for word in filtered_tokens if not 'http' in word]\n",
    "\n",
    "    #stemming\n",
    "    if language == 'en':\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_tokens = [stemmer.stem(word) if (word[0] != '#' and word[0] != '@' ) else word for word in filtered_no_url]\n",
    "        return stemmed_tokens\n",
    "    if language == 'german':\n",
    "        return filtered_no_url\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(cursor, collection):\n",
    "    for tweet in cursor:\n",
    "        if tweet['text_en'] is not None:\n",
    "            processed_text = preprocess_tweet(tweet['text_en'], 'en')\n",
    "        else:\n",
    "            processed_text = \"\"\n",
    "        \n",
    "        collection.update_one({\"id\":tweet[\"id\"]}, \n",
    "                                { '$set': { 'processed_text': processed_text}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_translation(cursor,collection):\n",
    "    temp_list = []\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,reduce_len=True,strip_handles=False)\n",
    "    client_secret = 'db36d42fd17b43bbbacacbaf545e513c'\n",
    "    auth_client = AzureAuthClient(client_secret)\n",
    "    bearer_token = b'Bearer ' + auth_client.get_access_token()\n",
    "    finalToken = bearer_token\n",
    "    headers = {\"Authorization \": finalToken}\n",
    "    \n",
    "    for tweet in cursor:\n",
    "        tokens = tokenizer.tokenize(tweet['text'])\n",
    "        filtered_no_url = [word for word in tokens if not 'http' in word]\n",
    "        \n",
    "        text_to_translate = \" \".join(filtered_no_url)\n",
    "        \n",
    "        translateUrl = \"http://api.microsofttranslator.com/v2/Http.svc/Translate?text={}&to={}\".format(text_to_translate, 'en')\n",
    "        translationData = requests.get(translateUrl, headers = headers)\n",
    "        # parse xml return values\n",
    "        translation = ElementTree.fromstring(translationData.text.encode('utf-8'))\n",
    "        temp_list.append((tweet[\"id\"],translation))\n",
    "    \n",
    "    pickle.dump( temp_list, open( \"translations.p\", \"wb\" ) )\n",
    "    return temp_list\n",
    "                \n",
    "    #collection.update_one({\"id\":tweet[\"id\"]}, { '$set': { 'text_en': translation}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open(r'new_clusters.txt') as f:\\n    data = f.readlines()\\n    \\ndataset = set(data[0][2:].split(','))\\ndataset.union(data[1][2:].split(','))\\ndataset.union(data[2][2:].split(','))\\n\\ntopic = []\\n\\nfor tweet in cursor:\\n    if tweet['id_str'] in dataset:\\n        topic.append(tweet)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = get_collection([\"mongodb://igenie_readwrite:igenie@35.189.101.142:27017/dax_gcp\",\"dax_gcp\", \"tweets\"])\n",
    "cursor = collection.find({'constituent':\"BMW\"},{\"_id\":-1, \"id_str\":1,\"favorite_count\":1, \n",
    "                    \"retweet_count\":1,\"text\":1, \"processed_text\":1, \"place\":1, \"user\":1})\n",
    "'''\n",
    "with open(r'new_clusters.txt') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "dataset = set(data[0][2:].split(','))\n",
    "dataset.union(data[1][2:].split(','))\n",
    "dataset.union(data[2][2:].split(','))\n",
    "\n",
    "topic = []\n",
    "\n",
    "for tweet in cursor:\n",
    "    if tweet['id_str'] in dataset:\n",
    "        topic.append(tweet)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vocabulary, top_retweeted, top_fav, top_followed, prices = analytics(cursor)\n",
    "vocabulary, top_retweeted, top_fav, top_followed, prices = analytics(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in reversed(top_fav):\n",
    "    pprint.pprint(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in reversed(top_followed):\n",
    "    pprint.pprint(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in top_retweeted:\n",
    "    pprint.pprint(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_vocabulary = sorted(vocabulary, key=vocabulary.__getitem__, reverse=True)\n",
    "for i in range(0,41):\n",
    "    print(\"{},{}\".format(sorted_vocabulary[i], vocabulary[sorted_vocabulary[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial 56\n",
      "Sales 877\n",
      "Bearish 33\n",
      "Recall 397\n",
      "Bullish 34\n",
      "56028\n"
     ]
    }
   ],
   "source": [
    "for key in vocabulary.keys():\n",
    "    if key == 'recal':\n",
    "        print(\"Recall\",vocabulary[key])\n",
    "    if key == 'sale':\n",
    "        print('Sales',vocabulary[key])\n",
    "    if key == 'financi':\n",
    "        print('Financial',vocabulary[key])\n",
    "    if key == 'bearish':\n",
    "        print('Bearish',vocabulary[key])\n",
    "    if key == 'bullish':\n",
    "        print('Bullish',vocabulary[key])\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average price: 88.30473559793396\n",
      "Highest price: 149.0\n",
      "Lowest price: 50.75\n"
     ]
    }
   ],
   "source": [
    "print_prices(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in cursor:\n",
    "    tweet['date'] = datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S %z %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor.sort(key=lambda d: d['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-18 08:37:50+00:00\n",
      "2017-08-01 15:36:20+00:00\n"
     ]
    }
   ],
   "source": [
    "print(cursor[0]['date'])\n",
    "print(cursor[-1][\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('US', 20), ('GB', 6), ('IN', 2), ('ID', 2), ('NL', 1), ('ZA', 1), ('FR', 1), ('AU', 1), ('NO', 1)]\n"
     ]
    }
   ],
   "source": [
    "countries = defaultdict(int)\n",
    "\n",
    "for tweet in list(cursor):\n",
    "    for word in tweet[\"processed_text\"]:\n",
    "        try:\n",
    "            number = float(word)\n",
    "            if number < 100 and number > 50:\n",
    "                if tweet['place'] is not None:\n",
    "                    if 'country_code' in tweet['place'].keys():\n",
    "                        countries[tweet['place']['country_code']] += 1\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print((sorted(countries.items(), key=lambda x:x[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices_frequency = defaultdict(int)\n",
    "for p in prices:\n",
    "    prices_frequency[p] += 1\n",
    "\n",
    "#print(print((sorted(prices_frequency.items(), key=lambda x:x[1], reverse=True))))\n",
    "for p1,p2 in sorted(prices_frequency.items(), key=lambda x:x[1], reverse=True):\n",
    "    print(\"{}\".format(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
